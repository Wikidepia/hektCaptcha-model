{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UwvgmEgtfCX"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "id": "3UwvgmEgtfCX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiO9Dflue2OT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install onnx onnxruntime onnxsim magic-wormhole\n",
        "!pip install torchsampler\n",
        "!pip install git+https://github.com/ildoonet/cutmix\n",
        "!wget https://raw.githubusercontent.com/davda54/sam/main/sam.py"
      ],
      "id": "XiO9Dflue2OT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21a773a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch import nn\n",
        "from torchvision.models import MobileNet_V3_Small_Weights, MobileNet_V3_Large_Weights\n",
        "\n",
        "# Define the device to use for computation (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MobileNetV3 model\n",
        "model = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
        "model.classifier = nn.Identity()\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ],
      "id": "21a773a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64d3101b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, labels_dir, transform):\n",
        "        self.image_paths = []\n",
        "        self.labels =[]\n",
        "        for key, label_dir in labels_dir.items():\n",
        "            for image_dir in glob(os.path.join(label_dir, '*.png')):\n",
        "                self.image_paths.append(image_dir)\n",
        "                self.labels.append(key)\n",
        "            for image_dir in glob(os.path.join(label_dir, '*.jpg')):\n",
        "                self.image_paths.append(image_dir)\n",
        "                self.labels.append(key)\n",
        "        self.transform = transform\n",
        "        self.image_size = 160\n",
        "        self.cutout_p = 0.1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.image_paths[idx], self.labels[idx]\n",
        "        image = Image.open(image_path)\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        image = self.transform(image, size=self.image_size, cutout_p=self.cutout_p)\n",
        "        return image, label\n",
        "\n",
        "    def next_epoch(self):\n",
        "      self.image_size = min(224, self.image_size + 16)\n",
        "      self.cutout_p = min(0.5, self.cutout_p + 0.15)\n",
        "\n",
        "    def get_labels(self):\n",
        "      return self.labels"
      ],
      "id": "64d3101b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-DL0D4LcNP3"
      },
      "outputs": [],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_channels, out_channels),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(out_channels, num_classes),\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Linear):\n",
        "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "id": "_-DL0D4LcNP3"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SoftTargetCrossEntropy(nn.Module):\n",
        "    def forward(self, x, target):\n",
        "        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "vCfq_zJBx7iy"
      },
      "id": "vCfq_zJBx7iy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKJiOgaNNQ91"
      },
      "source": [
        "## Training"
      ],
      "id": "GKJiOgaNNQ91"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Structure\n",
        "\n",
        "Here is data structure that are used:\n",
        "\n",
        "```\n",
        "train\n",
        "└── class_name\n",
        "    ├── yes\n",
        "    │   └── abc.png\n",
        "    └── no\n",
        "        └── abc.png\n",
        "```\n",
        "\n",
        "You can send data easily using magic-wormhole or upload it manually."
      ],
      "metadata": {
        "id": "XWaLIiqeRbyi"
      },
      "id": "XWaLIiqeRbyi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1zbx__5sEH8"
      },
      "outputs": [],
      "source": [
        "!mkdir train\n",
        "!cd train && wormhole receive 15-december-prowler --accept-file"
      ],
      "id": "s1zbx__5sEH8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fsKM6kmdzja"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms as T\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "def preprocess_image(image, size, cutout_p):\n",
        "  preprocess = T.Compose([\n",
        "        T.RandomResizedCrop(size=size, interpolation=InterpolationMode.BILINEAR),\n",
        "        T.ColorJitter(brightness=0.2, saturation=0.15, contrast=0.15, hue=0.1),\n",
        "        T.TrivialAugmentWide(interpolation=InterpolationMode.BILINEAR),\n",
        "        T.PILToTensor(),\n",
        "        T.ConvertImageDtype(torch.float),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        T.RandomErasing(p=cutout_p, value=\"random\"),\n",
        "  ])\n",
        "  return preprocess(image)"
      ],
      "id": "5fsKM6kmdzja"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f388b709"
      },
      "outputs": [],
      "source": [
        "from cutmix.cutmix import CutMix\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsampler import ImbalancedDatasetSampler\n",
        "\n",
        "from sam import SAM\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 15\n",
        "CLASS_NAME = \"clothing\"\n",
        "\n",
        "labels_dir = {0: f\"/content/train/{CLASS_NAME}/no\", 1: f\"/content/train/{CLASS_NAME}/yes\"}\n",
        "dataset = CustomImageDataset(labels_dir, preprocess_image)\n",
        "dataset = CutMix(dataset, num_class=len(labels_dir), beta=1.0, prob=0.2, num_mix=1)\n",
        "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                              sampler=ImbalancedDatasetSampler(dataset.dataset),\n",
        "                              pin_memory=True, num_workers=4)"
      ],
      "id": "f388b709"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY44DEaTmcVP"
      },
      "outputs": [],
      "source": [
        "classifier = Classifier(in_channels=576, out_channels=1024, num_classes=len(labels_dir)).to(device)\n",
        "criterion = SoftTargetCrossEntropy()\n",
        "\n",
        "optimizer = SAM(classifier.parameters(), torch.optim.AdamW, lr=3e-4, rho=1.0, adaptive=True)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.base_optimizer,\n",
        "                                                       T_max=(len(dataset) // BATCH_SIZE) * 2)"
      ],
      "id": "XY44DEaTmcVP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b7f8a67"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # Set the model to training mode\n",
        "    classifier.to(device).train()\n",
        "\n",
        "    # Initialize the running loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for inputs, targets in tqdm(train_dataloader):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        # Zero the optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "          emb_inputs = model(inputs)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = classifier(emb_inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # First Forward-Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.first_step(zero_grad=True)\n",
        "\n",
        "        # Second Forward-Backward\n",
        "        outputs = classifier(emb_inputs)\n",
        "        criterion(outputs, targets).backward()\n",
        "        optimizer.second_step(zero_grad=True)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Compute the batch accuracy and update the running accuracy and loss\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        running_accuracy += torch.argmax(targets, dim=1).eq(predicted).sum().item()\n",
        "        running_loss += loss.item() * len(targets)\n",
        "        num_samples += len(targets)\n",
        "\n",
        "    # Compute the average training loss and accuracy\n",
        "    train_loss = running_loss / num_samples\n",
        "    train_accuracy = running_accuracy / num_samples\n",
        "    if epoch % 3 == 0:\n",
        "      dataset.dataset.next_epoch()\n",
        "\n",
        "    # Print the epoch loss and accuracy\n",
        "    print(\"Epoch {} - Train Loss: {:.4f} - Train Accuracy: {:.4f}\".format(\n",
        "        epoch, train_loss, train_accuracy))"
      ],
      "id": "6b7f8a67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e66d128"
      },
      "source": [
        "## ONNX Export"
      ],
      "id": "1e66d128"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8032f59a"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.onnx.export(classifier.eval(),\n",
        "                      torch.randn(1, 576).to(device),\n",
        "                      f\"{CLASS_NAME}.onnx\",\n",
        "                      opset_version=17,\n",
        "                      do_constant_folding=True,\n",
        "                      input_names = ['input'],\n",
        "                      output_names = ['output'])"
      ],
      "id": "8032f59a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EUiGZx_DkHa"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_fp32 = f\"{CLASS_NAME}.onnx\"\n",
        "model_quant = f\"{CLASS_NAME}.onnx\"\n",
        "quantized_model = quantize_dynamic(model_fp32, model_quant)"
      ],
      "id": "4EUiGZx_DkHa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IK2_FnOg88b"
      },
      "outputs": [],
      "source": [
        "!onnxsim \"{CLASS_NAME}.onnx\" \"{CLASS_NAME}.onnx\""
      ],
      "id": "7IK2_FnOg88b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25a4d852",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!python3 -m onnxruntime.tools.convert_onnx_models_to_ort  \"{CLASS_NAME}.onnx\" --optimization_style Fixed\n",
        "!rm \"{CLASS_NAME}.required_operators.config\""
      ],
      "id": "25a4d852"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73075edf"
      },
      "source": [
        "## Post Training"
      ],
      "id": "73075edf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkiS75h9hkKP"
      },
      "outputs": [],
      "source": [
        "!wormhole send \"{CLASS_NAME}.ort\""
      ],
      "id": "LkiS75h9hkKP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-CV9zGbz-JU"
      },
      "source": [
        "## Testing"
      ],
      "id": "4-CV9zGbz-JU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "419af0b1"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "test_preprocess = T.Compose([\n",
        "    T.Resize(size=224, interpolation=InterpolationMode.BILINEAR),\n",
        "    T.ConvertImageDtype(torch.float),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "id": "419af0b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f0a3a40"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from torchvision.io import read_image\n",
        "import time\n",
        "\n",
        "classifier.eval()\n",
        "for image_file in glob(\"train/bicycles/no/*.*\"):\n",
        "    # Load the input image\n",
        "    input_image = read_image(image_file)\n",
        "\n",
        "    # Preprocess the input image\n",
        "    input_tensor = test_preprocess(input_image)\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    with torch.no_grad():\n",
        "      emb_inputs = model(input_batch)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "      outputs = classifier(emb_inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    if predicted == 1:\n",
        "      display(Image.open(image_file))\n",
        "      print(predicted, probs)\n",
        "    time.sleep(0.01)"
      ],
      "id": "5f0a3a40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKovfiTU-u0P"
      },
      "outputs": [],
      "source": [
        "!rm train/vinyl/no/bb92e4cdd878c292.*"
      ],
      "id": "VKovfiTU-u0P"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1e66d128",
        "4-CV9zGbz-JU"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}